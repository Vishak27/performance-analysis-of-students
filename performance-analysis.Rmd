---
title: "Eda_Project"
author: "performance_analysis_of_students"
date: "2023-03-27"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
df_csv = read.csv("data_csv.csv",header = T)
na.omit(df_csv)
df_csv
```

//optimal clusters
```{r}
library(factoextra)
```

```{r}
dfx = data.frame(g1 = df_csv$G1)

fviz_nbclust(dfx,kmeans,method = "wss",k.max = 10) #For finding the correct number 
                                                   #of clusters required for kmeans
```

//k means
```{r}
km.res=kmeans(dfx, 3, iter.max = 10, nstart = 1)


print(km.res)
```

```{r}
aggregate(dfx, by=list(cluster=km.res$cluster), mean)

```

```{r}

dd <- cbind(dfx, cluster = km.res$cluster)
dd
```

################################################################################

Decision Tree

```{r}
library(datasets)
library(caTools)
library(party)
library(dplyr)
library(magrittr)
library(e1071)

dfx2 = df_csv[,c(2,6,17)]
head(dfx2)
```

```{r}
dfx2$famsup = factor(dfx2$famsup)
dfx2$Pstatus = factor(dfx2$Pstatus) 
dfx2$sex = factor(dfx2$sex)
sample_data = sample.split(dfx2, SplitRatio = 0.8)
train_data <- subset(dfx2, sample_data == TRUE)
test_data <- subset(dfx2, sample_data == FALSE)

class(dfx2$sex)

```
//decision tree
```{r}
model<- ctree(famsup ~ sex + Pstatus, train_data)
plot(model)

```

//confusion matrix
```{r}
library(caret)
predict_model<-predict(ctree(famsup ~ sex + Pstatus, train_data), test_data)

# creates a table to count how many are classified
# as native speakers and how many are not
m_at <- table(test_data$famsup, predict_model)
m_at
```

//accuracy
```{r}
ac_Test = sum(diag(m_at)) / sum(m_at)
print(paste('Accuracy for test is found to be', ac_Test*100))

```

//corrplot
```{r}
stud_por=read.csv("data_csv.csv",header = T)
stud_por_df=data.frame(stud_por)
mydata.cor=cor(stud_por_df[,unlist(lapply(stud_por_df,is.numeric))])
```

```{r}
library(corrplot)
```

```{r}
corrplot(mydata.cor)
palette=colorRampPalette(c("green","white","red"))(20)
```

```{r}
heatmap(x=mydata.cor,col=palette,symm=TRUE)
```

```{r}
ggplot(stud_por_df,aes(x=G3))+geom_bar()
```

//svm

```{r}
set.seed(123)
assignment=sample(1:3,size=nrow(stud_por_df),prob=c(70,15,15),replace=TRUE)
stud_por_train=stud_por_df[assignment==1,]
stud_por_valid=stud_por_df[assignment==2,]
stud_por_test=stud_por_df[assignment==3,]
regressor=svm(formula=G3~ .,data=stud_por_train,cost=1)
regressor
```

```{r}
pred=predict(object=regressor,newdata=stud_por_test)
pred
```
```{r}
errors=stud_por_test$G3-pred
me=mean(errors*errors)
rmse=sqrt(me)
rmse
```

```{r}
library(ggplot2)
library(dplyr)
library(Hmisc)
library(cowplot)
library(WVPlots)
set.seed(123)
Data <- read.csv("data_csv.csv",header = T)

```

```{r}
#CORELATION BTW G3, G1& G2
x <- ggplot(Data, aes(G1, G3)) +
  geom_jitter(color = "blue", alpha = 0.5) +
  theme_light()

y <- ggplot(Data, aes(G2, G3)) +
  geom_jitter(color = "green", alpha = 0.5) +
  theme_light()

p <- plot_grid(x, y) 
title <- ggdraw() + draw_label("1. Correlation", fontface='bold')
plot_grid(title, p, ncol=1, rel_heights=c(0.1, 1))
```


```{r}
#CORELATION BTW G3, STUDY TIME & FREETIME
x <- ggplot(Data, aes(freetime, G3)) +
  geom_jitter(color = "blue", alpha = 0.5) +
  theme_light()

y <- ggplot(Data, aes(studytime, G3)) +
  geom_jitter(color = "green", alpha = 0.5) +
  theme_light()

p <- plot_grid(x, y) 
title <- ggdraw() + draw_label("1. Correlation", fontface='bold')
plot_grid(title, p, ncol=1, rel_heights=c(0.1, 1))

```


```{r}
#LINEAR REGRESSION MODEL

n_train <- round(0.8 * nrow(Data))
train_indices <- sample(1:nrow(Data), n_train)
Data_train <- Data[train_indices, ]
Data_test <- Data[-train_indices, ]

formula_0 <- as.formula("G3 ~ age + Medu + Fedu  + traveltime + studytime + failures + famrel+ freetime+ goout + Dalc + Walc + health + absences + G1 +G2 ")
model_0 <- lm(formula_0, data = Data_train)
summary(model_0)
```


```{r}
r_sq_0 <- summary(model_0)$r.squared

#predict data on test set
prediction_0 <- predict(model_0, newdata = Data_test)
#calculating the residuals
residuals_0 <- Data_test$charges - prediction_0
#calculating Root Mean Squared Error
rmse_0 <- sqrt(mean(residuals_0^2))

formula_1 <- as.formula("G3 ~ age + Medu + Fedu  + traveltime + studytime + failures + famrel+ freetime+ goout + Dalc + Walc + health + absences + G1 +G2 ")

model_1 <- lm(formula_1, data = Data_train)
summary(model_1)
```



```{r}
r_sq_1 <- summary(model_1)$r.squared

prediction_1 <- predict(model_1, newdata = Data_test)

residuals_1 <- Data_test$charges - prediction_1
rmse_1 <- sqrt(mean(residuals_1^2))

```


```{r}
#MODEL PERFORMANCE
Data_test$prediction <- predict(model_1, newdata = Data_test)
ggplot(Data_test, aes(x = prediction, y = G3)) + 
  geom_point(color = "blue", alpha = 0.7) + 
  geom_abline(color = "red") +
  ggtitle("Prediction vs. Real values")

```

```{r}
Data_test$residuals <- Data_test$G3 - Data_test$prediction

ggplot(data = Data_test, aes(x = prediction, y = residuals)) +
  geom_pointrange(aes(ymin = 0, ymax = residuals), color = "blue", alpha = 0.7) +
  geom_hline(yintercept = 0, linetype = 3, color = "red") +
  ggtitle("Residuals vs. Linear model prediction")
```

```{r}
ggplot(Data_test, aes(x = residuals)) + 
  geom_histogram(bins = 15, fill = "blue") +
  ggtitle("Histogram of residuals")
```

```{r}
GainCurvePlot(Data_test, "prediction", "G3", "Model")
```